\documentclass{article}
\usepackage[final]{nips_2017}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption}
\usepackage[ruled,vlined]{algorithm2e}
 \usepackage{amsmath}

\title{Waste Object Detection and Classification}

\author{
 Hrushikesh N. Kulkarni\\
  Department of Computer Science\\
  Stanford University\\
  \texttt{hrushi@stanford.edu} \\
  %% examples of more authors
  \And
  Nandini Kannamangalam Sundara Raman\\
  Department of Computer Science\\
  Stanford University\\
  \texttt{nandini9@stanford.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\begin{center}
\includegraphics[width=3cm, height=0.7cm]{CS230}
\end{center}

\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}	
Improper waste management has severe effects on our environment, natural resources and public health. In our project we take pictures of waste items and output semantic labels detecting and classifying the objects.
In this report, we experimented with different sizes of ResNet, and choose ResNet34 as it gave optimal accuracy of 93\%. Later for object localization, we used Faster RCNN with Inception V2 as the backbone network.

\section{Related work}

\citep{yang2016classification} and \cite{chu2018multilayer} in describe network architectures based on AlexNet. We found the following limitations in these approaches: Images used for testing in \citep{yang2016classification} and \cite{chu2018multilayer} have only one object per image.  
Misclassification of certain classes of objects (e.g., misclassifying cylindrical objects as recyclable objects). 
Cenk Bircanoglu et. al in \cite{bircanouglu2018recyclenet} uses MobileNet, a streamlined architecture which can be used in smart devices. This idea is useful as we also want to deploy our network on smartphones. 



\section{Dataset and Features}
We use TrashNet \citep{yang2016classification} dataset as the baseline. This dataset has 6 labelled classes (Glass, Paper, Cardboard, Plastic, Metal, Trash). We augment this dataset by adding more categories (mentioned in Problem Statement) to it.  After we collect more data (by taking photos with our smartphones) and ensure that our data is preprocessed to match the resolution of TrashNet[4] dataset. 



\section{ Methods }

\subsection{Generating Collages}

\subsubsection{Random Collage Generation}
We have a hand labeled collage generated. 



%Add figure over here

\subsubsection{Learned Collage}
% Add 2 branch network
Here we try to pose the problem of generating collage as a Knapsack problem. We want to optimize the collage such that the overlap between the two images is minimized. To do this we first get foreground masks of the original image. Since the images in the dataset are not centered, we register all the images such that the bounding box starts at the top left corner. We find out the $t_{x}$ and $t_{y}$  from the top left corner of the bounding box. Later by affine transform, we translate the image such that the top left corner is $(0,0)$.  

\begin{equation}
\begin{bmatrix}
x'\\
y'
\end{bmatrix}
= 
\begin{bmatrix}
1 & 0 & -t_{x}\\
0 & 1 & -t_{y}
\end{bmatrix}
\begin{bmatrix}
x\\
y\\
1
\end{bmatrix}
\end{equation}

\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.2\linewidth}
    \includegraphics[width=\linewidth]{data/images/section_4/base_original.png}
     \caption{Original Image}
  \end{subfigure}
  \begin{subfigure}[b]{0.2\linewidth}
    \includegraphics[width=\linewidth]{data/images/section_4/original_mask.png}
    \caption{Registered mask}
  \end{subfigure}
  \begin{subfigure}[b]{0.2\linewidth}
    \includegraphics[width=\linewidth]{data/images/section_4/original_target.png}
    \caption{Image to collage}
  \end{subfigure}
  \begin{subfigure}[b]{0.2\linewidth}
    \includegraphics[width=\linewidth]{data/images/section_4/target_mask.png}
    \caption{Registered mask}
  \end{subfigure}
  \caption{Collecting the data to set up the problem}
  \label{fig:coffee3}
\end{figure}


Now we define a collage loss, so that we have minimum overlap and maximum union area between the foreground. 
\begin{equation}
intersection = base_{mask} \bigcap target_{mask}
\end{equation}

\begin{equation}
union = base_{mask} \bigcup target_{mask}
\end{equation}

\begin{equation}
collage_{loss} = \frac{intersection}{union}
\end{equation}

For every ground truth, we define following anchor points by defining a grid over the image, and then finding the above loss function for each of these points. 

\begin{equation}
anchor_{loss} = 
\begin{bmatrix}
l_{1} l_{2} l_{3} \ldots 
\end{bmatrix}
\end{equation}

\begin{equation}
    optimal_{anchor} = argmin(anchor_{loss})
\end{equation}

We use this optimal anchor point as the ground truth for our learning the optimal place for placing the target. A sample output looks like this 

\begin{figure}

    \begin{subfigure}{0.2\linewidth}
    \includegraphics[width=\linewidth]{data/images/section_4/optimization_network.png}
     \caption{Optimization Network}
  \end{subfigure}
  \begin{subfigure}{\linewidth}
    \includegraphics[width=\linewidth]{data/images/section_4/collage_optimization_output.png}
    \caption{Collage of learned network}
  \end{subfigure}
 
    \label{fig:my_label}
\end{figure}


% Learning homography transforms using STN

\subsubsection{Blending collages}
% Add GP-GAN over here

\subsection{Individual Images}

\section{Experiments/Results/Discussion}

\subsection{Learning Rates}

\begin{itemize}
    \item LR-Find
    \item LL Decay
    \item Reduce Learning Rate on Plateau
\end{itemize}






%From format ------- TO BE REMOVED
You should also give details about what (hyper)parameters you chose (e.g. why did you
use X learning rate for gradient descent, what was your mini-batch size and why) and how
you chose them. What your primary metrics are: accuracy, precision,
AUC, etc. Provide equations for the metrics if necessary. For results, you want to have a
mixture of tables and plots. If you are solving a classification problem, you should include a
confusion matrix or AUC/AUPRC curves. Include performance metrics such as precision,
recall, and accuracy. For regression problems, state the average error. You should have
both quantitative and qualitative results. To reiterate, you must have both quantitative
and qualitative results! If it applies: include visualizations of results, heatmaps,
examples of where your algorithm failed and a discussion of why certain algorithms failed
or succeeded. In addition, explain whether you think you have overfit to your training set
and what, if anything, you did to mitigate that. Make sure to discuss the figures/tables in
your main text throughout this section. Your plots should include legends, axis labels, and
have font sizes that are legible when printed.
%------- TO BE REMOVED



\section{Conclusion/Future Work }
%From format ------- TO BE REMOVED
Summarize your report and reiterate key points. Which algorithms were the highestperforming?
Why do you think that some algorithms worked better than others? For
future work, if you had more time, more team members, or more computational resources,
what would you explore?
%------- TO BE REMOVED



\section{Contributions}
%From format ------- TO BE REMOVED
The contributions section is not included in the 5 page limit. This section should describe
what each team member worked on and contributed to the project.
%------- TO BE REMOVED



\section*{References}

%From format ------- TO BE REMOVED
This section should include citations for: (1) Any papers mentioned in the related work
section. (2) Papers describing algorithms that you used which were not covered in class.
(3) Code or libraries you downloaded and used. This includes libraries such as scikit-learn, Tensorflow, Pytorch, Keras etc. Acceptable formats include: MLA, APA, IEEE. If you
do not use one of these formats, each reference entry must include the following (preferably
in this order): author(s), title, conference/journal, publisher, year. If you are using TeX,
you can use any bibliography format which includes the items mentioned above. We are excluding
the references section from the page limit to encourage students to perform a thorough
literature review/related work section without being space-penalized if they include more
references. Any choice of citation style is acceptable
as long as you are consistent. 
%------- TO BE REMOVED

\medskip


\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}