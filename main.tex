\documentclass{article}
\usepackage[final]{nips_2017}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{amsmath}
\usepackage[numbers]{natbib}


\title{Waste Object Detection and Classification}

\author{
 Hrushikesh N. Kulkarni\\
  Department of Computer Science\\
  Stanford University\\
  \texttt{hrushi@stanford.edu} \\
  %% examples of more authors
  \And
  Nandini Kannamangalam Sundara Raman\\
  Department of Computer Science\\
  Stanford University\\
  \texttt{nandini9@stanford.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\begin{center}
\includegraphics[width=3cm, height=0.7cm]{CS230}
\end{center}

\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}	
Improper waste management has severe effects on our environment, natural resources and public health. We want to educate users to be more mindful of recycling throwaway items, so that we can reduce the contamination at the source. We use pictures of different throwaway items from \cite{yang2016classification} and train a classifier to output a label which is a type of the object. Knowing the labels, the user can make a sound decision if it has to be recycled. 

In our report we discuss innovative ways to augment the dataset, and present a object detection mechanism which as been trained on different throwaway classes. We also experiment different hyper-parameters for these learning algorithm and present a summary.

\section{Related work}

Object detection and classification approaches for throwaway items is a well studied topic. \citeauthor{yang2016classification} and \citeauthor{chu2018multilayer} use a AlexNet \cite{krizhevsky2012imagenet} like architectures, and have very poor accuracy. \cite{yang2016classification}, their classifier was confused between plastic and glass categories. 

To have more robust classification we experiment with different classifiers notebely ResNet by \citeauthor{he2016deep}, Inception by \citeauthor{szegedy2017inception}, Xception and Xception by \citeauthor{chollet2017xception}. Because of the skip connection mechanism in \cite{he2016deep} we find that the ResNet worked the best. 


% \citep{yang2016classification} and \cite{chu2018multilayer} in describe network architectures based on AlexNet. But they have We found the following limitations in these approaches: Images used for testing in \citep{yang2016classification} and \cite{chu2018multilayer} have only one object per image.  
% Misclassification of certain classes of objects (e.g., misclassifying cylindrical objects as recyclable objects). 
% Cenk Bircanoglu et. al in \cite{bircanouglu2018recyclenet} uses MobileNet, a streamlined architecture which can be used in smart devices. This idea is useful as we also want to deploy our network on smartphones. 



\section{Dataset and Features}
We use TrashNet \citep{yang2016classification} dataset as the baseline. This is a balanced dataset and has approx 400 images of 6 different labelled classes (Glass, Paper, Cardboard, Plastic, Metal, Trash).  We divide the dataset into train \/ validation \/ test as (80/10/10). The resolution of the input data is $512\times384\times3$.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{data/images/03_Dataset/train_sample.png}
    \caption{Sample Training Set from the TrashNet Dataset \cite{yang2016classification}}
    \label{fig:TrainSample}
\end{figure}

We augment this dataset by adding operations like flipping, rotation, sheering, etc.  Since the dataset has only one object in a image, we augument this by making a array of collage as mentioned in Sec. \ref{sec:GeneratingCollage}. 


\subsection{Multiple Objects in a Image}
To increase the quantity of the dataset, we generate a collage of multiple of these input images. We present three methods of generating collages below. 
\begin{itemize}
    \item Placing images at random
    \item Placing images explicitly
    \item Learning where to place the images
\end{itemize}

% \subsubsection{Random Collage Generation}
% We place individual images on top of each other at random location. Sample images are in Fig: \ref{fig:random_collage}

% \begin{figure}[h!]
%     \centering
%     \begin{subfigure}{0.3\linewidth}
%     \includegraphics[width=\linewidth]{data/images/04_random_collage/Output_1.png}
%     \end{subfigure}
%     \begin{subfigure}{0.3\linewidth}
%     \includegraphics[width=\linewidth]{data/images/04_random_collage/Output_2.png}
%     \end{subfigure}
%     \label{fig:random_collage}
%      \caption{Making collages with images placed randomly on top of each other}
% \end{figure}

\subsubsection{Placing images explicitly}
We explicitly place images at 4 quadrants. 

\begin{figure}[h!]
    \centering
    \begin{subfigure}{0.3\linewidth}
    \includegraphics[width=\linewidth]{data/images/04_random_collage/Output_1.png}
    \end{subfigure}
    \begin{subfigure}{0.3\linewidth}
    \includegraphics[width=\linewidth]{data/images/04_random_collage/Output_2.png}
    \end{subfigure}
    \label{fig:random_collage}
     \caption{ADD EXPLICIT IMAGES OVER HERE}
\end{figure}

\subsubsection{Learned Collage}
\label{sec:GeneratingCollage}
% Add 2 branch network
We pose the problem of generating collages as a Knapsack problem \cite{KnapsackProblem}. We optimize the collage such that overlap between two images is minimized. Let us consider that we have to make a collage of two images $A$ and $B$ such that $B$ is on top of $A$. Then we find the foreground mask of the two images $A_{mask}$ and $B_{mask}$. We register all individual images such that the bounding box of 

\begin{algorithm}[H]
\SetAlgoLined
Two images $A$ and $B$ \\
Find the respective foreground masks $A_{mask}$ and $B_{mask}$\\
Translate $A_{mask}$ and $B_{mask}$ such that\\
$B_{A} =  Boundingbox(A_{mask})$\\
$B_{B} =  Boundingbox(B_{mask})$\\
$Topleft(B_{A}) = (0,0) $\\
$Topleft(B_{B}) = (0,0) $\\
 \caption{Registering images}
\end{algorithm}

Now once all the images are registered, we convolve images $B_{mask}$ over  $A_{mask}$ collage loss as given in Eq. \eqref{eq:collage_loss} at every location. To save on compute, we divide the image in equidistant anchor points shown by intersection of grind points as Fig. \ref{fig:anchorpoints}. 


Now by translating $B_{mask}$ at each anchor point, we can find the anchor loss at each anchor point as given in \ref{eq:anchorloss}.

\begin{equation}
\begin{bmatrix}
x'_{B_{mask}}\\
y'_{B_{mask}}
\end{bmatrix}
= 
\begin{bmatrix}
1 & 0 & t_{x}\\
0 & 1 & t_{y}
\end{bmatrix}
\begin{bmatrix}
x_{B_{mask}}\\
y_{B_{mask}}\\
1
\end{bmatrix}
\end{equation}


\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=\linewidth]{data/images/04_Learned_Collage/base_original.png}
     \caption{Image $A$}
  \end{subfigure}
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=\linewidth]{data/images/04_Learned_Collage/original_mask.png}
    \caption{Registered $A_{mask}$}
  \end{subfigure}
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=\linewidth]{data/images/04_Learned_Collage/Anchor.png}
    \caption{Anchor points of $A_{mask}$}
    \label{fig:anchorpoints}
  \end{subfigure}
  
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=\linewidth]{data/images/04_Learned_Collage/original_target.png}
    \caption{Image B}
  \end{subfigure}
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=\linewidth]{data/images/04_Learned_Collage/target_mask.png}
    \caption{Registered $B_{mask}$}
  \end{subfigure}
  \caption{Collecting the data to set up the problem}
  \label{fig:learning_collages}
\end{figure}


Now we define a collage loss, so that we have minimum overlap and maximum union area between the foreground. 
\begin{equation}
intersection = A_{mask} \bigcap B_{mask}
\end{equation}

\begin{equation}
union = A_{mask} \bigcup B_{mask}
\end{equation}

\begin{equation}
collage_{loss} = \frac{intersection}{union}
\label{eq:collage_loss}
\end{equation}

\begin{equation}
anchor_{loss} = 
\begin{bmatrix}
l_{1}, l_{2}, l_{3}, \ldots 
\end{bmatrix}
\label{eq:anchorloss}
\end{equation}

We collect the ground truth by exhaustively searching the anchor points for all the images in dataset for $A$ and $B$.

\begin{equation}
    B_{anchor} = argmin(anchor_{loss})
\end{equation}
We use the median of all such points to indicate the location of optimal placement with respect to image $A$

\begin{equation}
A_{optimal} = median
\begin{bmatrix}
 B_{anchor}, C_{anchor}, D_{anchor}, \ldots 
\end{bmatrix}
\label{eq:anchorloss}
\end{equation}

\pageref{fig:learned_collage_output}


\begin{figure}
    \begin{subfigure}{0.5\linewidth}
    \includegraphics[ width=\linewidth]{data/images/04_Learned_Collage/Optimization_Network_Hand.png}
     \caption{Optimization Network}
  \end{subfigure}
  \begin{subfigure}{0.5\linewidth}
    \includegraphics[width=0.7\linewidth]{data/images/04_Learned_Collage/collage_optimization_output.png}
    \caption{Collage of learned network}
  \end{subfigure}
    \label{fig:learned_collage_output}
    \caption{Learned network and sample output collage}
\end{figure}

To resize the image at first, we start with a max pooling layer, and then followed by a LeNet like architecture. This approach was promising at first, it did not work for the following reasons. 

\begin{itemize}
    \item Using all registered images, cause the output anchor point to be always the same. Hence all generated collages looked very similar. 
    \item It took too much time to calculate the ground thruth. 
\end{itemize}



% Learning homography transforms using STN

\subsubsection{Blending Images using GP-GAN}
To blend different masks on a collage we experimented with a pre-trained GP-GAN \cite{wu2019gp}. This approach uses Gaussian-Poisson Generative Adversarial Network (GP-GAN), which poses the joint optimization between the gradient and image color information, see Fig. \ref{fig:GP-GAN_Arch}. We used pretrained weights available from \cite{wu2019gp}, and sample output can be seen in Fig. \ref{fig:GAN_Blending}. We decided not to go in favour of this approach, since it blurs image features we suspected that it will hurt the performance.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{data/images/04_GP_GAN/GP_GAN.png}
    \caption{Architecture of the GP-GAN \cite{wu2019gp}. }
    \label{fig:GP-GAN_Arch}
\end{figure}


\begin{figure}
  \centering
  \begin{subfigure}[b]{0.2\linewidth}
    \includegraphics[width=\linewidth]{data/images/04_GP_GAN/cardboard254.jpg}
     \caption{Image $A$ }
  \end{subfigure}
  \begin{subfigure}[b]{0.2\linewidth}
    \includegraphics[width=\linewidth]{data/images/04_GP_GAN/mask_display.png}
    \caption{Mask}
  \end{subfigure}
  \begin{subfigure}[b]{0.2\linewidth}
    \includegraphics[width=\linewidth]{data/images/04_GP_GAN/metal14.jpg}
    \caption{Image $B$}
  \end{subfigure}
  \begin{subfigure}[b]{0.2\linewidth}
    \includegraphics[width=\linewidth]{data/images/04_GP_GAN/result-cardboard331-metal14.png}
    \caption{Result}
  \end{subfigure}
  \caption{Using GP-GAN \cite{wu2019gp} to blend images $A$ and $B$ using a Mask.   }
  \label{fig:GAN_Blending}
\end{figure}



\section{ Methods }
In this section, we describe different ways to generate a collage, and methods for classifying and localizing objects in a image. 




%Add figure over here



\subsection{Classification of Objects}
TrashNet \cite{yang2016classification} reports only 63\% accuracy for classification on the test set, this can be mainly attributed to its comparatively simple CNN model. We reproduced the results from \cite{CollinChing} where ResNet34 was used to do classification of objects in TrashNet dataset  \cite{yang2016classification} and got better results than \cite{CollinChing}. 

As ResNet \cite{he2016deep} performs better on image classification tasks, we applied transfer learning on ResNet models with weights pre-trained on ImageNet dataset as a baseline. We froze all the layers except the fully connected layers at the top, and the network was finetuned to TrashNet dataset. 

We experimented with different number of layers of ResNet. We noticed that using deeper ResNet does not yield significant improvements in test set accuracy and concluded that ResNet34 was a good sweet spot. 

We used FastAI \cite{howard2018fastai} library to search for an optimum learning rate and fit using one cycle optimization algorithm \cite{1CyclePolicy}. The plots of losses and other metrics are shown below.



\subsection{Hybrid Transfer Learning}
\label{HybridTransferLearning}
We use a Hybrid approach to Transfer Learning from \cite{geron2019hands}. For trying this out, we start with pretrained models on image net. Then add a Global Average Pooling layer, followed by Batch Normalization, then Dense output layer. We start by freezing all base layers, and training the output layer by a large learning rate, then after a few epochs, we reduce the learning rate drastically.

\begin{algorithm}[H]
\SetAlgoLined
\KwResult{Learned Model }
 base =  pretrained model \;
 
 avg = GlobalAveragePooling(base) \\
 reg = BatchNormalization(avg)
 output = Dense(number of classes, activation="softmax", input=reg)

 Freeze base layers of the downloaded model\;

 \While{epochs < 10}{
 train the model with a very high learning rate; 
 }
 
 Unfreeze all base layers; 
 
 \While{epochs < 100}{
 train the model with a very low learning rate; 
 }
 \caption{Hybrid transfer learning}
\end{algorithm}

The advantage of this approach is that it does not change the weights of the base layers drastically in the first few epochs when the last few layers have not yet stabilized. 

\section{Experiments/Results/Discussion}

\subsection{Learning Rates}

\begin{itemize}
    \item LR-Find
    \item LL Decay
    \item Reduce Learning Rate on Plateau. We reduce the learning rate whenever the validation score reaches a plateau. \ref{fig:reduce_lr}
    \item Learning Rate Schedule
\end{itemize}

\begin{figure}
    \centering
    
    \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=\linewidth]{data/images/05_Comparision/Learning_rate_finder.png}
    \caption{Finding Optimal Learning Rate Using \cite{1CyclePolicy} }
    \label{fig:1CyclePolicy}
    \end{subfigure}
    \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=\linewidth]{data/images/05_Results/Lower_Lr_Plateau.png}
    \caption{Reduce the learning rate by $0.2$}
    \label{fig:reduce_lr}
    \end{subfigure}
    
\end{figure}


We try different learning rates for \ref{HybridTransferLearning}. 

\begin{tabular}{|c|c|c|c|}
\hline
& $lr_{1}=0.01$, $lr_{2}=0.01$ & $lr_{1}=0.001$, $lr_{2}=0.001$  &   $lr_{1}=0.01$, $lr_{2}=0.0001$ \\
\hline
 Precision    & 0.15        & 0.06  & \\
 Recall       &  0.91       & 0.16  & \\
 F1-Score     & 0.10        & 0.07  & \\
 \hline
\end{tabular}

%From format ------- TO BE REMOVED
You should also give details about what (hyper)parameters you chose (e.g. why did you
use X learning rate for gradient descent, what was your mini-batch size and why) and how
you chose them. What your primary metrics are: accuracy, precision,
AUC, etc. Provide equations for the metrics if necessary. For results, you want to have a
mixture of tables and plots. If you are solving a classification problem, you should include a
confusion matrix or AUC/AUPRC curves. Include performance metrics such as precision,
recall, and accuracy. For regression problems, state the average error. You should have
both quantitative and qualitative results. To reiterate, you must have both quantitative
and qualitative results! If it applies: include visualizations of results, heatmaps,
examples of where your algorithm failed and a discussion of why certain algorithms failed
or succeeded. In addition, explain whether you think you have overfit to your training set
and what, if anything, you did to mitigate that. Make sure to discuss the figures/tables in
your main text throughout this section. Your plots should include legends, axis labels, and
have font sizes that are legible when printed.
%------- TO BE REMOVED



\section{Conclusion/Future Work }
%From format ------- TO BE REMOVED
Summarize your report and reiterate key points. Which algorithms were the highestperforming?
Why do you think that some algorithms worked better than others? For
future work, if you had more time, more team members, or more computational resources,
what would you explore?
%------- TO BE REMOVED



\section{Contributions}
%From format ------- TO BE REMOVED
The contributions section is not included in the 5 page limit. This section should describe
what each team member worked on and contributed to the project.
%------- TO BE REMOVED



\section*{References}

%From format ------- TO BE REMOVED
This section should include citations for: (1) Any papers mentioned in the related work
section. (2) Papers describing algorithms that you used which were not covered in class.
(3) Code or libraries you downloaded and used. This includes libraries such as scikit-learn, Tensorflow, Pytorch, Keras etc. Acceptable formats include: MLA, APA, IEEE. If you
do not use one of these formats, each reference entry must include the following (preferably
in this order): author(s), title, conference/journal, publisher, year. If you are using TeX,
you can use any bibliography format which includes the items mentioned above. We are excluding
the references section from the page limit to encourage students to perform a thorough
literature review/related work section without being space-penalized if they include more
references. Any choice of citation style is acceptable
as long as you are consistent. 
%------- TO BE REMOVED

\medskip


\bibliographystyle{apalike} %other styles produce (author?)
\bibliography{references}

\end{document}